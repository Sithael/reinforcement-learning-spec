{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataclasses\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install dataclasses\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(5):                                         # play game for 5 episodes\n",
    "#    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#    while True:\n",
    "#        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#        dones = env_info.local_done                        # see if episode finished\n",
    "#        scores += env_info.rewards                         # update the score (for each agent)\n",
    "#        states = next_states                               # roll over states to next time step\n",
    "#        if np.any(dones):                                  # exit loop if episode finished\n",
    "#            break\n",
    "#    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, output_file=None):\n",
    "    episodes = len(scores)\n",
    "    x = np.arange(1, episodes + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, scores, linewidth=2, marker='o', markersize=6, linestyle='-')\n",
    "\n",
    "    plt.title(\"Agent's Performance over Episodes\", fontsize=18, fontweight='bold')\n",
    "    plt.xlabel('Episodes', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Scores', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedReplayBuffer:\n",
    "    def __init__(self, max_size, critic_dims, actor_dims, \n",
    "            n_actions, n_agents, batch_size):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.n_agents = n_agents\n",
    "        self.actor_dims = actor_dims\n",
    "        self.critic_dims = critic_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, self.critic_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, self.critic_dims), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros((self.mem_size, self.n_agents), dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros((self.mem_size, self.n_agents), dtype=np.bool)\n",
    "\n",
    "        self.init_actor_memory()\n",
    "\n",
    "    def init_actor_memory(self):\n",
    "        self.actor_state_memory = []\n",
    "        self.actor_new_state_memory = []\n",
    "        self.actor_action_memory = []\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            self.actor_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_new_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_action_memory.append(\n",
    "                            np.zeros((self.mem_size, self.n_actions)))\n",
    "\n",
    "\n",
    "    def store_transition(self, raw_obs, state, action, reward, \n",
    "                               raw_obs_, state_, done):\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.actor_state_memory[agent_idx][index] = raw_obs[agent_idx]\n",
    "            self.actor_new_state_memory[agent_idx][index] = raw_obs_[agent_idx]\n",
    "            self.actor_action_memory[agent_idx][index] = action[agent_idx]\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        actor_states = []\n",
    "        actor_new_states = []\n",
    "        actions = []\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            actor_states.append(self.actor_state_memory[agent_idx][batch])\n",
    "            actor_new_states.append(self.actor_new_state_memory[agent_idx][batch])\n",
    "            actions.append(self.actor_action_memory[agent_idx][batch])\n",
    "\n",
    "        return actor_states, states, actions, rewards, \\\n",
    "               actor_new_states, states_, terminal\n",
    "\n",
    "    def ready(self):\n",
    "        if self.mem_cntr >= self.batch_size:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, critic_lr, input_dims, fc1_dims, fc2_dims, \n",
    "                    n_agents, n_actions):\n",
    "        super().__init__()\n",
    "        self.critic_lr = critic_lr\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.n_agents = n_agents\n",
    "        self.fc1 = nn.Linear(self.input_dims+self.n_agents*self.n_actions, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.q.weight)\n",
    "        \n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.q.bias.data.zero_()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.critic_lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(T.cat([state, action], dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, actor_lr, input_dims, fc1_dims, fc2_dims, \n",
    "                 n_actions):\n",
    "        super().__init__()\n",
    "        self.actor_lr = actor_lr\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.mu.weight)\n",
    "        \n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.mu.bias.data.zero_()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.actor_lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.mu(x))\n",
    "\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mu=0.0, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actor_dims, critic_dims, n_actions, n_agents, agent_idx,\n",
    "                    actor_lr, critic_lr, fc1, \n",
    "                    fc2, gamma, tau, epsilon, epsilon_decay):\n",
    "        \n",
    "        self.actor_dims = actor_dims\n",
    "        self.critic_dims = critic_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.n_agents = n_agents\n",
    "        self.agent_idx = agent_idx\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.fc1 = fc1\n",
    "        self.fc2 = fc2\n",
    "        self.agent_name = 'agent_%s' % self.agent_idx\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.actor = ActorNetwork(self.actor_lr, self.actor_dims, self.fc1, self.fc2, self.n_actions)\n",
    "        self.critic = CriticNetwork(self.critic_lr, self.critic_dims, self.fc1, self.fc2, self.n_agents, self.n_actions)\n",
    "        self.target_actor = ActorNetwork(self.actor_lr, self.actor_dims, self.fc1, self.fc2, self.n_actions)\n",
    "        self.target_critic = CriticNetwork(self.critic_lr, self.critic_dims, self.fc1, self.fc2, self.n_agents, self.n_actions)\n",
    "        \n",
    "        self.noise = OUActionNoise(mu=np.zeros(self.n_actions))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def save(self, model_prefix):\n",
    "        T.save(\n",
    "        {\n",
    "            'local_actor': self.actor.state_dict(),\n",
    "            'target_actor': self.target_actor.state_dict(),\n",
    "            'local_critic': self.critic.state_dict(),\n",
    "            'target_critic': self.target_critic.state_dict(),\n",
    "        },\n",
    "        f'./{model_prefix}.pt')\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(state)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            mu_prime = mu + T.tensor(self.noise(), dtype=T.float).to(self.actor.device)\n",
    "        else:\n",
    "            mu_prime = mu\n",
    "            \n",
    "        self.actor.train()\n",
    "            \n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return mu_prime.detach().cpu().numpy()[0]\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        actor_params = self.actor.named_parameters()\n",
    "\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_actor_state_dict[name].clone()\n",
    "\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_critic_state_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, config):\n",
    "        self.actor_dims = config.actor_dims\n",
    "        self.critic_dims = config.critic_dims\n",
    "        self.n_agents = config.n_agents\n",
    "        self.n_actions = config.n_actions\n",
    "        self.scenario = config.scenario\n",
    "        self.actor_lr = config.actor_lr\n",
    "        self.critic_lr = config.critic_lr\n",
    "        self.fc1 = config.fc1_dims\n",
    "        self.fc2 = config.fc2_dims\n",
    "        self.gamma = config.gamma\n",
    "        self.tau = config.tau\n",
    "        self.epsilon = config.epsilon\n",
    "        self.epsilon_decay = config.epsilon_decay\n",
    "        self.agents = []\n",
    "        \n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.agents.append(Agent(self.actor_dims[agent_idx], self.critic_dims,  \n",
    "                            self.n_actions, self.n_agents, agent_idx,\n",
    "                            actor_lr=self.actor_lr, critic_lr=self.critic_lr, \n",
    "                            fc1=self.fc1, fc2=self.fc2, gamma=self.gamma, tau=self.tau,\n",
    "                            epsilon=self.epsilon, epsilon_decay=self.epsilon_decay))\n",
    "\n",
    "    def choose_action(self, raw_obs):\n",
    "        actions = []\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            action = agent.choose_action(raw_obs[agent_idx])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def learn(self, memory):\n",
    "        if not memory.ready():\n",
    "            print('GATHERING DATA')\n",
    "            return\n",
    "        \n",
    "        actor_states, states, actions, rewards, \\\n",
    "        actor_new_states, states_, dones = memory.sample_buffer()\n",
    "\n",
    "        device = self.agents[0].actor.device\n",
    "\n",
    "        states = T.tensor(states, dtype=T.float).to(device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(device)\n",
    "        rewards = T.tensor(rewards, dtype=T.float).to(device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(device)\n",
    "        dones = T.tensor(dones.astype(np.uint8)).to(device)\n",
    "\n",
    "        all_agents_new_actions = []\n",
    "        all_agents_new_mu_actions = []\n",
    "        old_agents_actions = []\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            new_states = T.tensor(actor_new_states[agent_idx], \n",
    "                                 dtype=T.float).to(device)\n",
    "\n",
    "            new_pi = agent.target_actor.forward(new_states)\n",
    "\n",
    "            all_agents_new_actions.append(new_pi)\n",
    "            mu_states = T.tensor(actor_states[agent_idx], \n",
    "                                 dtype=T.float).to(device)\n",
    "            pi = agent.actor.forward(mu_states)\n",
    "            all_agents_new_mu_actions.append(pi)\n",
    "            old_agents_actions.append(actions[agent_idx])\n",
    "\n",
    "        new_actions = T.cat([acts for acts in all_agents_new_actions], dim=1)\n",
    "        mu = T.cat([acts for acts in all_agents_new_mu_actions], dim=1)\n",
    "        old_actions = T.cat([acts for acts in old_agents_actions],dim=1)\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            #critic_value_ = agent.target_critic.forward(states_, new_actions).flatten() # torch 0.4.0 :(\n",
    "            critic_value_ = agent.target_critic.forward(states_, new_actions).view(-1)\n",
    "            critic_value_[dones[:,0]] = 0.0\n",
    "            #critic_value = agent.critic.forward(states, old_actions).flatten() # torch 0.4.0 :(\n",
    "            critic_value = agent.critic.forward(states, old_actions).view(-1)\n",
    "\n",
    "            target = rewards[:,agent_idx] + agent.gamma*critic_value_\n",
    "            critic_loss = F.mse_loss(target, critic_value)\n",
    "            agent.critic.optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            agent.critic.optimizer.step()\n",
    "\n",
    "            #actor_loss = agent.critic.forward(states, mu).flatten() # torch 0.4.0 :(\n",
    "            actor_loss = agent.critic.forward(states, mu).view(-1)\n",
    "            actor_loss = -T.mean(actor_loss)\n",
    "            agent.actor.optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            agent.actor.optimizer.step()\n",
    "\n",
    "            agent.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    n_agents: int\n",
    "    actor_dims: int\n",
    "    critic_dims: int\n",
    "    actor_lr: float\n",
    "    critic_lr: float\n",
    "    tau: float\n",
    "    n_actions: int\n",
    "    input_dims: tuple\n",
    "    gamma: float\n",
    "    mem_size: int\n",
    "    batch_size: float\n",
    "    fc1_dims: int\n",
    "    fc2_dims: int\n",
    "    scenario: str\n",
    "    epsilon: float\n",
    "    epsilon_decay: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_vector(observation):\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_actor_and_critic_obs_dims(observation, n_agents):\n",
    "    actor_dims = []\n",
    "    for i in range(n_agents):\n",
    "        actor_dims.append(observation[i].shape[0])\n",
    "    critic_dims = sum(actor_dims)\n",
    "    return actor_dims, critic_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launcher(env, actions, episodes=100, model_prefix='model'):\n",
    "    brain_name = env.brain_names[0]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    observation_shape = env_info.vector_observations[0].shape\n",
    "    n_agents = len(env_info.agents)\n",
    "    actor_dims, critic_dims = fetch_actor_and_critic_obs_dims(env_info.vector_observations, n_agents)\n",
    "\n",
    "    pipeline_config = PipelineConfig(\n",
    "        n_agents=n_agents,\n",
    "        actor_dims=actor_dims,\n",
    "        critic_dims=critic_dims,\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-3,\n",
    "        tau=1e-3,\n",
    "        n_actions=actions,\n",
    "        input_dims=tuple(observation_shape),\n",
    "        gamma=0.99,\n",
    "        mem_size=int(1e6),\n",
    "        batch_size=256,\n",
    "        fc1_dims=128,\n",
    "        fc2_dims=128,\n",
    "        scenario='competition',\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=1e-6,\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "    best_score = -np.inf\n",
    "    global_timestep = 0\n",
    "    maddpg = MADDPG(config=pipeline_config)\n",
    "    memory = SharedReplayBuffer(pipeline_config.mem_size, pipeline_config.critic_dims, pipeline_config.actor_dims, \n",
    "                    pipeline_config.n_actions, pipeline_config.n_agents, pipeline_config.batch_size) \n",
    "\n",
    "    for episode in range(episodes):\n",
    "        score = 0\n",
    "        dones = [False] * n_agents\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        observations = env_info.vector_observations  # Get observations for all agents\n",
    "\n",
    "        while True:\n",
    "            actions = maddpg.choose_action(observations)  # Choose actions for all agents\n",
    "            clipped_actions = np.clip(actions, -1, 1)           # all actions between -1 and 1\n",
    "            env_info = env.step(clipped_actions)[brain_name]\n",
    "\n",
    "            observations_ = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            observations_vector = create_state_vector(observations)\n",
    "            next_observations_vector = create_state_vector(observations_)\n",
    "\n",
    "            score += np.sum(rewards)\n",
    "            memory.store_transition(observations, observations_vector, actions, rewards, observations_,\n",
    "                                next_observations_vector, dones)\n",
    "            maddpg.learn(memory)\n",
    "\n",
    "            observations = observations_\n",
    "            global_timestep += 1\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            # Save the model for each agent\n",
    "            for i, agent in enumerate(maddpg.agents):\n",
    "                agent.save(f'agent_no{i}_{model_prefix}')\n",
    "\n",
    "            best_score = avg_score\n",
    "        \n",
    "        if (episode % 100 == 0) and (episode > 0):\n",
    "            print(f\"Episode: {episode}, Best score: {best_score}, Avg score: {avg_score}, Global timestep: {global_timestep}\")\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = launcher(env, actions=action_size, episodes=4_000, model_prefix='model')\n",
    "plot_scores(scores, output_file='agent_performance_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('scores.npy', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
